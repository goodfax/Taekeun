{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec<br><br>\n",
    "\n",
    "#### using gensim\n",
    "    https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 10:44:42,908 : INFO : collecting all words and their counts\n",
      "2020-01-21 10:44:42,922 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-01-21 10:44:42,928 : INFO : collected 3315 word types from a corpus of 16658 raw words and 6 sentences\n",
      "2020-01-21 10:44:42,928 : INFO : Loading a fresh vocabulary\n",
      "2020-01-21 10:44:42,931 : INFO : effective_min_count=5 retains 413 unique words (12% of original 3315, drops 2902)\n",
      "2020-01-21 10:44:42,932 : INFO : effective_min_count=5 leaves 12280 word corpus (73% of original 16658, drops 4378)\n",
      "2020-01-21 10:44:42,943 : INFO : deleting the raw counts dictionary of 3315 items\n",
      "2020-01-21 10:44:42,945 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2020-01-21 10:44:42,945 : INFO : downsampling leaves estimated 6708 word corpus (54.6% of prior 12280)\n",
      "2020-01-21 10:44:42,947 : INFO : estimated required memory for 413 words and 100 dimensions: 536900 bytes\n",
      "2020-01-21 10:44:42,948 : INFO : resetting layer weights\n",
      "2020-01-21 10:44:43,052 : INFO : training model with 7 workers on 413 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-01-21 10:44:43,060 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-21 10:44:43,063 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-21 10:44:43,064 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-21 10:44:43,065 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-21 10:44:43,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-21 10:44:43,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-21 10:44:43,086 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-21 10:44:43,088 : INFO : EPOCH - 1 : training on 16658 raw words (6741 effective words) took 0.0s, 212097 effective words/s\n",
      "2020-01-21 10:44:43,101 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-21 10:44:43,103 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-21 10:44:43,103 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-21 10:44:43,104 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-21 10:44:43,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-21 10:44:43,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-21 10:44:43,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-21 10:44:43,124 : INFO : EPOCH - 2 : training on 16658 raw words (6650 effective words) took 0.0s, 205621 effective words/s\n",
      "2020-01-21 10:44:43,134 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-21 10:44:43,136 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-21 10:44:43,136 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-21 10:44:43,137 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-21 10:44:43,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-21 10:44:43,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-21 10:44:43,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-21 10:44:43,157 : INFO : EPOCH - 3 : training on 16658 raw words (6659 effective words) took 0.0s, 231163 effective words/s\n",
      "2020-01-21 10:44:43,167 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-21 10:44:43,169 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-21 10:44:43,170 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-21 10:44:43,170 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-21 10:44:43,173 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-21 10:44:43,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-21 10:44:43,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-21 10:44:43,190 : INFO : EPOCH - 4 : training on 16658 raw words (6684 effective words) took 0.0s, 234448 effective words/s\n",
      "2020-01-21 10:44:43,199 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-21 10:44:43,203 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-21 10:44:43,203 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-21 10:44:43,204 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-21 10:44:43,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-21 10:44:43,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-21 10:44:43,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-21 10:44:43,223 : INFO : EPOCH - 5 : training on 16658 raw words (6720 effective words) took 0.0s, 228264 effective words/s\n",
      "2020-01-21 10:44:43,224 : INFO : training on a 83290 raw words (33454 effective words) took 0.2s, 195239 effective words/s\n",
      "2020-01-21 10:44:43,224 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-01-21 10:44:43,225 : INFO : saving Word2Vec object under ./model/taekeun/ko_sample.bin, separately None\n",
      "2020-01-21 10:44:43,226 : INFO : not storing attribute vectors_norm\n",
      "2020-01-21 10:44:43,227 : INFO : not storing attribute cum_table\n",
      "2020-01-21 10:44:43,269 : INFO : saved ./model/taekeun/ko_sample.bin\n"
     ]
    }
   ],
   "source": [
    "# model generation and save\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing as mp\n",
    "\n",
    "ko_fname = './data/prepro_ko_wiki_sample.txt'\n",
    "model_fname = './model/taekeun/ko_sample.bin'\n",
    "\n",
    "# corpus = [sent.strip().split(\" \") for sent in open(ko_fname, 'r', encoding='utf-8').readlines()]\n",
    "model = Word2Vec(LineSentence(ko_fname), size=100, workers=mp.cpu_count()-1, sg=1)\n",
    "model.save(model_fname)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 10:44:51,193 : INFO : loading Word2Vec object from ./model/taekeun/ko_sample.bin\n",
      "2020-01-21 10:44:51,198 : INFO : loading wv recursively from ./model/taekeun/ko_sample.bin.wv.* with mmap=None\n",
      "2020-01-21 10:44:51,200 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-01-21 10:44:51,200 : INFO : loading vocabulary recursively from ./model/taekeun/ko_sample.bin.vocabulary.* with mmap=None\n",
      "2020-01-21 10:44:51,201 : INFO : loading trainables recursively from ./model/taekeun/ko_sample.bin.trainables.* with mmap=None\n",
      "2020-01-21 10:44:51,202 : INFO : setting ignored attribute cum_table to None\n",
      "2020-01-21 10:44:51,203 : INFO : loaded ./model/taekeun/ko_sample.bin\n",
      "D:\\Anaconda3\\envs\\taekeun\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"\n",
      "2020-01-21 10:44:51,208 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('10', 0.9992877244949341), ('던', 0.9992845058441162), ('위기', 0.9992266893386841), ('세계', 0.9992055892944336), ('부터', 0.9991786479949951), ('그리스', 0.9991623163223267), ('선거', 0.9991573095321655), ('정보', 0.9991551041603088), ('이집트', 0.9991432428359985), ('관타나모', 0.9991422891616821)]\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load('./model/taekeun/ko_sample.bin')\n",
    "print(model.similar_by_word('지미'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation and save\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing as mp\n",
    "\n",
    "ko_fname = './data/prepro_ko_wiki.txt'\n",
    "model_fname = './model/taekeun/ko.bin'\n",
    "\n",
    "# corpus = [sent.strip().split(\" \") for sent in open(ko_fname, 'r', encoding='utf-8').readlines()]\n",
    "model = Word2Vec(LineSentence(ko_fname), size=100, workers=mp.cpu_count()-1, sg=1)\n",
    "model.save(model_fname)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec train result\n",
    "    - 총 331765개의 article을 학습\n",
    "    - input corpus에 들어갈 데이터를 배열의 형태로 직접 만들지 않고 gensim word2vec의 LinSentence 사용.(https://radimrehurek.com/gensim/models/word2vec.html, LineSentence 검색)\n",
    "    - mp.cpu_count(): 내 컴퓨터의 cpu 코어수\n",
    "    - cpu 코어 수를 최대로 사용하는 것과 일부만 사용하는 것이 학습속도에 별차이가 없음.\n",
    "<br>\n",
    "\n",
    "##### build dictionary for vocabulary\n",
    "![title](../_etc/ko_build_dic.PNG)\n",
    "<br>\n",
    "\n",
    "##### train start\n",
    "![title](../_etc/ko_train_start.PNG)\n",
    "<br>\n",
    "\n",
    "##### 1 epoch end\n",
    "![title](../_etc/ko_train_1epoch.PNG)\n",
    "<br>\n",
    "\n",
    "##### train end\n",
    "![title](../_etc/ko_train_end.PNG)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Continuum\\anaconda3\\envs\\taekeun\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('워맥', 0.8122756481170654), ('게리', 0.7990656495094299), ('팰런', 0.7989846467971802), ('데이브', 0.7973150014877319), ('프랭크', 0.7964510917663574), ('스털츠', 0.7951048612594604), ('몬티스', 0.7930748462677002), ('맥기네스', 0.7921020984649658), ('글렌', 0.7910065650939941), ('데이비드', 0.790729284286499)]\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load('./model/taekeun/ko.bin')\n",
    "print(model.similar_by_word('지미'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
